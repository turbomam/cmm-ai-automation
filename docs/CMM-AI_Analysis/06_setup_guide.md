# Chapter 6: Setup Guide

[<- Back to Index](00_index.md) | [Previous: API and LLM](05_api_and_llm.md) | [Next: Recommendations ->](07_recommendations.md)

---

## Critical Barriers (Will Cause Immediate Failure)

### 1. Missing Knowledge Graph Databases (~44GB of data)

**Impact**: Many scripts will fail or produce empty results

The KG analysis scripts require two large DuckDB databases that must be created from TSV source files:

| Database | Expected Location | Source Files | Size |
|----------|------------------|--------------|------|
| kg-microbe | `data/kgm/kg-microbe.duckdb` | `kg-microbe_nodes.tsv`, `kg-microbe_edges.tsv` | ~1.4M nodes |
| kg-microbe-function | `data/kgm/kg-microbe-function.duckdb` | `kg-microbe-function_nodes.tsv` (15GB), `kg-microbe-function_edges.tsv` (29GB) | ~151M nodes |

**Current status**: The `data/kgm/` directory does NOT exist in the repository.

**Scripts affected**:
- `src/media_search.py` - Falls back gracefully with warning
- `src/strain_search.py` - Falls back gracefully with warning
- `src/kg_analysis/*.py` - All will fail
- `src/kg_update_*.py` - All will fail
- Any Makefile target starting with `kg-`

### 2. Broken NCBI Entrez Email Configuration

**Impact**: All NCBI API calls will fail or be rate-limited aggressively

4 files have placeholder email that MUST be changed:

```python
Entrez.email = "your.email@example.com"  # PLACEHOLDER - WILL NOT WORK
```

**Files affected**:
- `src/ncbi_search.py:14`
- `src/transcriptomics_search.py:16`
- `src/strain_search.py:26`
- `src/add_missing_organisms.py:18`

**Makefile targets affected**:
- `update-genomes`
- `update-biosamples`
- `update-publications`
- `update-transcriptomics`
- `add-organisms`

**Fix**: Either edit the files directly, or set an environment variable and modify the code:
```python
import os
Entrez.email = os.environ.get("NCBI_EMAIL", "your.email@example.com")
if Entrez.email == "your.email@example.com":
    raise ValueError("Set NCBI_EMAIL environment variable")
```

### 3. Broken Makefile Paths (7 targets)

**Impact**: Commands will fail with "file not found"

These targets reference scripts without the `src/` prefix:

| Broken Target | Line |
|---------------|------|
| `update-pathways` | 111 |
| `update-datasets` | 119 |
| `update-genes` | 152 |
| `update-structures` | 160 |
| `update-publications` | 168 |
| `add-annotations` | 307 |
| `test` | 313 |

See [Chapter 4](04_code_analysis.md) for the fix.

---

## Configuration Required (Must Set Before Use)

### 4. API Keys for Planned Features

Not currently blocking, but these features won't work:

| API | Status | Where Configured |
|-----|--------|------------------|
| BacDive | NOT IMPLEMENTED | `strain_search.py:330` has TODO |
| protocols.io | NOT IMPLEMENTED | `assay_search.py:155` has TODO |

### 5. GitHub Secrets for CI/CD

Required for GitHub Actions but not for local use:

| Secret | Used By |
|--------|---------|
| `CBORG_API_KEY` | dragon-ai.yml, claude-issue-*.yml |
| `CLAUDE_CODE_OAUTH_TOKEN` | claude.yml |
| `ANTHROPIC_API_KEY` | claude-issue-*.yml (fallback) |
| `PAT_FOR_PR` | dragon-ai.yml |

---

## Data Files Required

### 6. Excel Source File

**Location**: `data/sheet/BER CMM Data for AI.xlsx`
**Status**: EXISTS (254KB)
**Required for**: `make convert-excel`, `make merge-excel`

### 7. PDF Publications Directory

**Location**: `data/publications/`
**Status**: EXISTS (contains ~20 PDFs + markdown conversions)
**Required for**: `make extend2`, PDF extraction workflows

### 8. TSV Data Files

**Location**: `data/txt/sheet/`
**Status**: Should be generated by `make convert-excel`
**Required for**: All `update-*` and `extend-*` targets

---

## Python Dependencies

### 9. Required Packages (from pyproject.toml)

```
pandas>=2.0.0
openpyxl>=3.0.0      # Excel reading
python-docx>=0.8.0   # Word documents
PyPDF2>=3.0.0        # PDF text extraction
pymupdf>=1.23.0      # Better PDF extraction (fitz)
biopython>=1.80      # NCBI Entrez
requests>=2.28.0     # HTTP calls
linkml-runtime>=1.7.0
linkml>=1.7.0
duckdb>=0.9.0        # KG database
```

**Install with**: `uv sync` or `make install`

### 10. Optional but Recommended

```
pytest>=7.0.0
ruff>=0.1.0
```

---

## Runtime Assumptions

### 11. Working Directory

Scripts assume they are run from repository root:
```python
Path("data/txt/sheet/BER_CMM_Data_for_AI_*.tsv")
```

Running from a different directory will cause path resolution failures.

### 12. Internet Connectivity

Required for:
- NCBI API calls
- KEGG REST API
- PubChem REST API
- ArrayExpress API
- UniProt API
- PDF downloads

### 13. Disk Space

- KG databases: ~44GB for source TSV files + DuckDB storage
- PDF downloads: Variable, currently ~10MB
- Extended TSV outputs: Minimal (<10MB)

---

## Quick Start Checklist

To run the basic pipeline from scratch:

```bash
# 1. Install dependencies
make install  # or: uv sync

# 2. Fix NCBI email (manual edit required)
# Edit these files and replace "your.email@example.com":
#   - src/ncbi_search.py
#   - src/transcriptomics_search.py
#   - src/strain_search.py
#   - src/add_missing_organisms.py

# 3. Convert Excel to TSV
make convert-excel

# 4. Check what's available
make status

# 5. Run basic extension (will skip KG features)
make update-all
```

---

## What Will Work Without KG Databases

These should work with just the basic setup:
- `make convert-excel` - Excel -> TSV conversion
- `make validate-consistency` - Data validation
- `make validate-schema` - LinkML schema validation
- `make status` - Check file existence
- Basic `update-*` targets (with NCBI email fixed)
- `make extend2` - PDF extraction workflow

## What Will NOT Work Without KG Databases

- `make kg-update-*` - All KG mining targets
- `make analyze-*` - All KG analysis targets (if targets were added)
- KG-Microbe node lookups in `media_search.py`, `strain_search.py`

---

## How to Convert KG TSV Files to DuckDB

### Expected File Locations

The scripts expect TSV files at these paths (relative to repo root):

**kg-microbe (phenotypic, smaller):**
```
data/kgm/kg-microbe_nodes.tsv
data/kgm/kg-microbe_edges.tsv
```

**kg-microbe-function (protein functions, larger ~44GB):**
```
data/kgm/kg-microbe-function_nodes.tsv   # 15GB, 151M nodes
data/kgm/kg-microbe-function_edges.tsv   # 29GB, 555M edges
```

### Commands to Create DuckDB Databases

Once TSV files are in place:

```bash
# Create the kgm directory if needed
mkdir -p data/kgm

# For kg-microbe (smaller, should be quick):
uv run python -m src.kg_analysis.kg_database --create --stats

# For kg-microbe-function (larger, 15-30 min):
uv run python -m src.kg_analysis.kg_function_database --create --stats

# Or test with a sample first (1M rows only):
uv run python -m src.kg_analysis.kg_function_database --create --sample
```

### If TSV Files Are in a Different Location

**Option 1: Symlink to expected location**
```bash
ln -s /path/to/your/kg-microbe_nodes.tsv data/kgm/kg-microbe_nodes.tsv
ln -s /path/to/your/kg-microbe_edges.tsv data/kgm/kg-microbe_edges.tsv
```

**Option 2: Modify the Python class instantiation** (requires code changes or CLI args not currently exposed)

### What the Conversion Does

From `src/kg_analysis/kg_database.py` (lines 43-94):

1. Creates `data/kgm/` directory if missing
2. Loads nodes TSV into DuckDB table using `read_csv_auto()`
3. Loads edges TSV with lenient parsing (`null_padding=true`, `ignore_errors=true`)
4. Creates indexes on: `id`, `category`, `subject`, `object`, `predicate`
5. Reports node/edge counts

The function KG version (`kg_function_database.py`) is similar but warns about the 15-30 minute load time for the larger files.

### Output Files Created

```
data/kgm/kg-microbe.duckdb          # ~smaller, phenotypic data
data/kgm/kg-microbe-function.duckdb  # ~larger, protein functions
```

---

## Local Databases Queried

| Database | Location | Size | Usage |
|----------|----------|------|-------|
| kg-microbe | `data/kgm/kg-microbe.duckdb` | ~1.4M nodes | Phenotypic data |
| kg-microbe-function | `data/kgm/kg-microbe-function.duckdb` | ~151M nodes | Protein functions |

Both are queried via DuckDB SQL in `src/kg_analysis/` modules.

---

[<- Back to Index](00_index.md) | [Previous: API and LLM](05_api_and_llm.md) | [Next: Recommendations ->](07_recommendations.md)
